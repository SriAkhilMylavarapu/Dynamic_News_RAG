{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34786edc-3658-4e1b-8032-d02491d05855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import json\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c345796-fee5-4ef5-81ea-8747b6e4ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories for the project.\"\"\"\n",
    "    directories = [\n",
    "        \"data/papers/arxiv\",\n",
    "        \"data/explanations/wikipedia\",\n",
    "        \"data/explanations/simple_wikipedia\",\n",
    "        \"data/processed\"\n",
    "    ]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    print(\"Directory structure created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eafcb77a-a815-4059-ae7e-03945a83858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "\n",
    "def download_arxiv_papers(search_query, max_results=100, output_dir=\"data/papers/arxiv\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=search_query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    \n",
    "    for result in client.results(search):\n",
    "        try:\n",
    "            filename = f\"{output_dir}/{result.get_short_id().replace('/', '_')}.pdf\"\n",
    "            result.download_pdf(filename=filename)\n",
    "            \n",
    "            print(f\"Downloaded: {result.title}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {result.get_short_id()}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de771d9d-6bd4-49c9-8fb5-2d0867006bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\n",
      "Downloaded: All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
      "Downloaded: RITA: Group Attention is All You Need for Timeseries Analytics\n"
     ]
    }
   ],
   "source": [
    "download_arxiv_papers(\"Attention is all you need\", 3, \"papers_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042505a-32a9-4f4e-8dd0-26a618ef9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_cs_papers(max_results_per_category=20):\n",
    "    cs_categories = [\n",
    "        \"cs.AI\",    # Artificial Intelligence\n",
    "        \"cs.CL\",    # Computation and Language (NLP)\n",
    "        \"cs.CV\",    # Computer Vision\n",
    "        \"cs.LG\",    # Machine Learning\n",
    "        \"cs.SE\",    # Software Engineering\n",
    "        \"cs.HC\",    # Human-Computer Interaction\n",
    "        \"cs.DB\",    # Databases\n",
    "        \"cs.NE\",    # Neural and Evolutionary Computing\n",
    "        \"cs.SD\",    # Sound (Audio processing)\n",
    "        \"cs.IR\"     # Information Retrieval\n",
    "    ]\n",
    "    \n",
    "    client = arxiv.Client(page_size=100, delay_seconds=3)\n",
    "    \n",
    "    all_papers = []\n",
    "    \n",
    "    for category in cs_categories:\n",
    "        print(f\"Downloading papers for category: {category}\")\n",
    "        output_dir = f\"data/papers/arxiv/{category}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        search = arxiv.Search(\n",
    "            query=f\"cat:{category}\",\n",
    "            max_results=max_results_per_category,\n",
    "            sort_by=arxiv.SortCriterion.Relevance\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            results = list(client.results(search))\n",
    "            print(f\"Found {len(results)} papers in {category}\")\n",
    "            \n",
    "            for result in results:\n",
    "                try:\n",
    "                    paper_id = result.get_short_id().replace('/', '_')\n",
    "                    filename = f\"{output_dir}/{paper_id}.pdf\"\n",
    "                    \n",
    "                    result.download_pdf(filename=filename)\n",
    "                    \n",
    "                    paper_info = {\n",
    "                        \"id\": result.get_short_id(),\n",
    "                        \"title\": result.title,\n",
    "                        \"authors\": [author.name for author in result.authors],\n",
    "                        \"abstract\": result.summary,\n",
    "                        \"category\": category,\n",
    "                        \"published\": str(result.published),\n",
    "                        \"pdf_path\": filename\n",
    "                    }\n",
    "                    all_papers.append(paper_info)\n",
    "                    \n",
    "                    print(f\"Downloaded: {paper_info['id']} - {paper_info['title']}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error downloading {result.get_short_id()}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching category {category}: {e}\")\n",
    "    \n",
    "    with open(\"data/papers/arxiv_cs_papers_metadata.json\", \"w\") as f:\n",
    "        json.dump(all_papers, f, indent=2)\n",
    "    \n",
    "    print(f\"Downloaded {len(all_papers)} papers in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3037f-2b33-4e00-a4f2-c8c53ae29f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            max_pages = min(20, len(pdf_reader.pages)) #max 20 pages\n",
    "            for page_num in range(max_pages):\n",
    "                text += pdf_reader.pages[page_num].extract_text() or \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94681c51-5c87-496b-a0b8-34d605502b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_concepts(text, top_n=15):\n",
    "    if not text or len(text) < 100:\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        max_length = min(len(text), 25000)\n",
    "        doc = nlp(text[:max_length])\n",
    "        \n",
    "        noun_phrases = [chunk.text.lower() for chunk in doc.noun_chunks \n",
    "                       if len(chunk.text.split()) > 1 and len(chunk.text) < 50]\n",
    "        \n",
    "        # Count occurrences and get most frequent\n",
    "        if not noun_phrases:\n",
    "            return []\n",
    "            \n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "        X = vectorizer.fit_transform([\" \".join(noun_phrases)])\n",
    "        \n",
    "        # Get top phrases\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        counts = X.toarray()[0]\n",
    "        \n",
    "        # Sort by frequency and filter out common non-technical terms\n",
    "        stopwords = [\"et al\", \"paper\", \"section\", \"figure\", \"table\", \"result\", \"method\", \"approach\"]\n",
    "        key_concepts = []\n",
    "        \n",
    "        for i in counts.argsort()[::-1]:\n",
    "            term = feature_names[i]\n",
    "            if not any(stop in term for stop in stopwords) and len(term) > 5:\n",
    "                key_concepts.append(term)\n",
    "            if len(key_concepts) >= top_n:\n",
    "                break\n",
    "                \n",
    "        return key_concepts\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting concepts: {e}\")\n",
    "        return []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
